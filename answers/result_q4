Commencing training!
COMMAND: train.py --save-dir /tmp/nlu_cw2/results/q4 --log-file /tmp/nlu_cw2/results/q4/log.out --data /tmp/nlu_cw2/europarl_prepared --encoder-num-layers 2 --decoder-num-layers 3
Arguments: {'data': '/tmp/nlu_cw2/europarl_prepared', 'source_lang': 'de', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 10, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 100, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 10, 'log_file': '/tmp/nlu_cw2/results/q4/log.out', 'save_dir': '/tmp/nlu_cw2/results/q4', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
Loaded a source dictionary (de) with 5047 words
Loaded a target dictionary (en) with 4420 words
Built a model with 1820164 parameters
Epoch 000: loss 5.797 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.16 | clip 0.981
Epoch 000: valid_loss 5.42 | num_tokens 13.8 | batch_size 500 | valid_perplexity 225
Epoch 001: loss 5.389 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.33 | clip 1
Epoch 001: valid_loss 5.19 | num_tokens 13.8 | batch_size 500 | valid_perplexity 179
Epoch 002: loss 5.22 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.78 | clip 1
Epoch 002: valid_loss 5.05 | num_tokens 13.8 | batch_size 500 | valid_perplexity 157
Epoch 003: loss 5.053 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.32 | clip 1
Epoch 003: valid_loss 4.91 | num_tokens 13.8 | batch_size 500 | valid_perplexity 135
Epoch 004: loss 4.882 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.23 | clip 1
Epoch 004: valid_loss 4.73 | num_tokens 13.8 | batch_size 500 | valid_perplexity 113
Epoch 005: loss 4.714 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.36 | clip 1
Epoch 005: valid_loss 4.58 | num_tokens 13.8 | batch_size 500 | valid_perplexity 97.9
Epoch 006: loss 4.576 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.39 | clip 1
Epoch 006: valid_loss 4.46 | num_tokens 13.8 | batch_size 500 | valid_perplexity 86.1
Epoch 007: loss 4.448 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.33 | clip 1
Epoch 007: valid_loss 4.35 | num_tokens 13.8 | batch_size 500 | valid_perplexity 77.4
Epoch 008: loss 4.338 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.39 | clip 1
Epoch 008: valid_loss 4.26 | num_tokens 13.8 | batch_size 500 | valid_perplexity 70.8
Epoch 009: loss 4.244 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.48 | clip 1
Epoch 009: valid_loss 4.19 | num_tokens 13.8 | batch_size 500 | valid_perplexity 65.7
Epoch 010: loss 4.164 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.64 | clip 1
Epoch 010: valid_loss 4.12 | num_tokens 13.8 | batch_size 500 | valid_perplexity 61.7
Epoch 011: loss 4.093 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.74 | clip 1
Epoch 011: valid_loss 4.06 | num_tokens 13.8 | batch_size 500 | valid_perplexity 58
Epoch 012: loss 4.028 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 16.94 | clip 1
Epoch 012: valid_loss 4.01 | num_tokens 13.8 | batch_size 500 | valid_perplexity 55.2
Epoch 013: loss 3.968 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.05 | clip 1
Epoch 013: valid_loss 3.97 | num_tokens 13.8 | batch_size 500 | valid_perplexity 52.9
Epoch 014: loss 3.914 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.23 | clip 1
Epoch 014: valid_loss 3.93 | num_tokens 13.8 | batch_size 500 | valid_perplexity 51
Epoch 015: loss 3.861 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.36 | clip 1
Epoch 015: valid_loss 3.89 | num_tokens 13.8 | batch_size 500 | valid_perplexity 49
Epoch 016: loss 3.813 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.51 | clip 1
Epoch 016: valid_loss 3.86 | num_tokens 13.8 | batch_size 500 | valid_perplexity 47.3
Epoch 017: loss 3.771 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.66 | clip 1
Epoch 017: valid_loss 3.84 | num_tokens 13.8 | batch_size 500 | valid_perplexity 46.4
Epoch 018: loss 3.728 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.76 | clip 1
Epoch 018: valid_loss 3.81 | num_tokens 13.8 | batch_size 500 | valid_perplexity 45
Epoch 019: loss 3.684 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 17.88 | clip 1
Epoch 019: valid_loss 3.78 | num_tokens 13.8 | batch_size 500 | valid_perplexity 43.9
Epoch 020: loss 3.65 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18 | clip 1
Epoch 020: valid_loss 3.76 | num_tokens 13.8 | batch_size 500 | valid_perplexity 42.8
Epoch 021: loss 3.611 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.09 | clip 1
Epoch 021: valid_loss 3.74 | num_tokens 13.8 | batch_size 500 | valid_perplexity 42
Epoch 022: loss 3.574 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.25 | clip 1
Epoch 022: valid_loss 3.72 | num_tokens 13.8 | batch_size 500 | valid_perplexity 41.2
Epoch 023: loss 3.541 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.35 | clip 1
Epoch 023: valid_loss 3.7 | num_tokens 13.8 | batch_size 500 | valid_perplexity 40.5
Epoch 024: loss 3.509 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.42 | clip 1
Epoch 024: valid_loss 3.69 | num_tokens 13.8 | batch_size 500 | valid_perplexity 39.9
Epoch 025: loss 3.48 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.58 | clip 1
Epoch 025: valid_loss 3.67 | num_tokens 13.8 | batch_size 500 | valid_perplexity 39.1
Epoch 026: loss 3.449 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.63 | clip 1
Epoch 026: valid_loss 3.65 | num_tokens 13.8 | batch_size 500 | valid_perplexity 38.5
Epoch 027: loss 3.416 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.76 | clip 1
Epoch 027: valid_loss 3.64 | num_tokens 13.8 | batch_size 500 | valid_perplexity 38.2
Epoch 028: loss 3.388 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.85 | clip 1
Epoch 028: valid_loss 3.63 | num_tokens 13.8 | batch_size 500 | valid_perplexity 37.6
Epoch 029: loss 3.362 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 18.96 | clip 1
Epoch 029: valid_loss 3.61 | num_tokens 13.8 | batch_size 500 | valid_perplexity 37
Epoch 030: loss 3.331 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.05 | clip 1
Epoch 030: valid_loss 3.6 | num_tokens 13.8 | batch_size 500 | valid_perplexity 36.7
Epoch 031: loss 3.303 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.28 | clip 1
Epoch 031: valid_loss 3.59 | num_tokens 13.8 | batch_size 500 | valid_perplexity 36.1
Epoch 032: loss 3.279 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.37 | clip 1
Epoch 032: valid_loss 3.57 | num_tokens 13.8 | batch_size 500 | valid_perplexity 35.7
Epoch 033: loss 3.258 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.36 | clip 1
Epoch 033: valid_loss 3.56 | num_tokens 13.8 | batch_size 500 | valid_perplexity 35.3
Epoch 034: loss 3.231 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.4 | clip 1
Epoch 034: valid_loss 3.56 | num_tokens 13.8 | batch_size 500 | valid_perplexity 35
Epoch 035: loss 3.21 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.54 | clip 1
Epoch 035: valid_loss 3.54 | num_tokens 13.8 | batch_size 500 | valid_perplexity 34.6
Epoch 036: loss 3.189 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.69 | clip 1
Epoch 036: valid_loss 3.54 | num_tokens 13.8 | batch_size 500 | valid_perplexity 34.5
Epoch 037: loss 3.166 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.73 | clip 1
Epoch 037: valid_loss 3.53 | num_tokens 13.8 | batch_size 500 | valid_perplexity 34.1
Epoch 038: loss 3.146 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.87 | clip 1
Epoch 038: valid_loss 3.52 | num_tokens 13.8 | batch_size 500 | valid_perplexity 33.7
Epoch 039: loss 3.125 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 19.87 | clip 1
Epoch 039: valid_loss 3.51 | num_tokens 13.8 | batch_size 500 | valid_perplexity 33.4
Epoch 040: loss 3.101 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20 | clip 1
Epoch 040: valid_loss 3.5 | num_tokens 13.8 | batch_size 500 | valid_perplexity 33.1
Epoch 041: loss 3.081 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.04 | clip 1
Epoch 041: valid_loss 3.49 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32.8
Epoch 042: loss 3.061 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.11 | clip 1
Epoch 042: valid_loss 3.49 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32.6
Epoch 043: loss 3.045 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.22 | clip 1
Epoch 043: valid_loss 3.48 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32.5
Epoch 044: loss 3.021 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.35 | clip 1
Epoch 044: valid_loss 3.47 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32.2
Epoch 045: loss 3.005 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.4 | clip 1
Epoch 045: valid_loss 3.46 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32
Epoch 046: loss 2.985 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.51 | clip 1
Epoch 046: valid_loss 3.46 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32
Epoch 047: loss 2.966 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.48 | clip 1
Epoch 047: valid_loss 3.45 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.6
Epoch 048: loss 2.945 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.63 | clip 1
Epoch 048: valid_loss 3.45 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.6
Epoch 049: loss 2.929 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.71 | clip 1
Epoch 049: valid_loss 3.45 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.5
Epoch 050: loss 2.913 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.84 | clip 1
Epoch 050: valid_loss 3.44 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.2
Epoch 051: loss 2.898 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.82 | clip 1
Epoch 051: valid_loss 3.44 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.1
Epoch 052: loss 2.875 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 20.93 | clip 1
Epoch 052: valid_loss 3.43 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.9
Epoch 053: loss 2.865 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.12 | clip 1
Epoch 053: valid_loss 3.43 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.8
Epoch 054: loss 2.846 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.04 | clip 1
Epoch 054: valid_loss 3.42 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.7
Epoch 055: loss 2.829 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.11 | clip 1
Epoch 055: valid_loss 3.42 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.7
Epoch 056: loss 2.817 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.24 | clip 1
Epoch 056: valid_loss 3.42 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.6
Epoch 057: loss 2.798 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.3 | clip 1
Epoch 057: valid_loss 3.41 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.3
Epoch 058: loss 2.787 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.39 | clip 1
Epoch 058: valid_loss 3.41 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.3
Epoch 059: loss 2.767 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.57 | clip 1
Epoch 059: valid_loss 3.41 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.2
Epoch 060: loss 2.758 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.63 | clip 1
Epoch 060: valid_loss 3.41 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.2
Epoch 061: loss 2.742 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.68 | clip 1
Epoch 061: valid_loss 3.4 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.1
Epoch 062: loss 2.726 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.68 | clip 1
Epoch 062: valid_loss 3.4 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30
Epoch 063: loss 2.718 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.78 | clip 0.999
Epoch 063: valid_loss 3.4 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.1
Epoch 064: loss 2.701 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.81 | clip 1
Epoch 064: valid_loss 3.4 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30
Epoch 065: loss 2.691 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.91 | clip 1
Epoch 065: valid_loss 3.4 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.9
Epoch 066: loss 2.673 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 21.92 | clip 1
Epoch 066: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.8
Epoch 067: loss 2.658 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.1 | clip 0.999
Epoch 067: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 068: loss 2.65 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.09 | clip 0.999
Epoch 068: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.8
Epoch 069: loss 2.633 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.11 | clip 1
Epoch 069: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.7
Epoch 070: loss 2.621 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.24 | clip 1
Epoch 070: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 071: loss 2.611 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.29 | clip 1
Epoch 071: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.5
Epoch 072: loss 2.601 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.32 | clip 1
Epoch 072: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 073: loss 2.587 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.38 | clip 1
Epoch 073: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 074: loss 2.578 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.4 | clip 1
Epoch 074: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.5
Epoch 075: loss 2.56 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.44 | clip 1
Epoch 075: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.7
Epoch 076: loss 2.55 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.57 | clip 1
Epoch 076: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.5
Epoch 077: loss 2.543 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.65 | clip 1
Epoch 077: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.4
Epoch 078: loss 2.53 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.67 | clip 0.999
Epoch 078: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.5
Epoch 079: loss 2.521 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.66 | clip 1
Epoch 079: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.4
Epoch 080: loss 2.502 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.74 | clip 1
Epoch 080: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 081: loss 2.497 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.84 | clip 1
Epoch 081: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.3
Epoch 082: loss 2.489 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.87 | clip 1
Epoch 082: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.2
Epoch 083: loss 2.479 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 22.91 | clip 0.999
Epoch 083: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.4
Epoch 084: loss 2.468 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.06 | clip 0.999
Epoch 084: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.3
Epoch 085: loss 2.455 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23 | clip 1
Epoch 085: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.4
Epoch 086: loss 2.448 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.02 | clip 1
Epoch 086: valid_loss 3.37 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.2
Epoch 087: loss 2.433 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.14 | clip 1
Epoch 087: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.2
Epoch 088: loss 2.425 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.17 | clip 1
Epoch 088: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.3
Epoch 089: loss 2.418 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.15 | clip 0.999
Epoch 089: valid_loss 3.37 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.2
Epoch 090: loss 2.405 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.27 | clip 1
Epoch 090: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.3
Epoch 091: loss 2.394 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.25 | clip 1
Epoch 091: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.3
Epoch 092: loss 2.384 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.44 | clip 0.999
Epoch 092: valid_loss 3.37 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.2
Epoch 093: loss 2.38 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.49 | clip 1
Epoch 093: valid_loss 3.37 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.2
Epoch 094: loss 2.371 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.51 | clip 0.999
Epoch 094: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.3
Epoch 095: loss 2.36 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.52 | clip 1
Epoch 095: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.5
Epoch 096: loss 2.349 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.56 | clip 1
Epoch 096: valid_loss 3.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.4
Epoch 097: loss 2.346 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.65 | clip 1
Epoch 097: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 098: loss 2.334 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.65 | clip 1
Epoch 098: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
Epoch 099: loss 2.324 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.65 | clip 0.998
Epoch 099: valid_loss 3.39 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29.6
[2021-02-19 04:18:39] COMMAND: translate.py --checkpoint-path /tmp/nlu_cw2/results/q4/checkpoint_best.pt --output /tmp/nlu_cw2/results/q4/model_translations.txt
[2021-02-19 04:18:39] Arguments: {'cuda': False, 'seed': 42, 'data': '/tmp/nlu_cw2/europarl_prepared', 'checkpoint_path': '/tmp/nlu_cw2/results/q4/checkpoint_best.pt', 'batch_size': 10, 'output': '/tmp/nlu_cw2/results/q4/model_translations.txt', 'max_len': 25, 'source_lang': 'de', 'target_lang': 'en', 'max_tokens': None, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 100, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 10, 'log_file': '/tmp/nlu_cw2/results/q4/log.out', 'save_dir': '/tmp/nlu_cw2/results/q4', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_num_layers': 2, 'decoder_num_layers': 3, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
[2021-02-19 04:18:39] Loaded a source dictionary (de) with 5047 words
[2021-02-19 04:18:39] Loaded a target dictionary (en) with 4420 words
[2021-02-19 04:18:39] Loaded a model from checkpoint /tmp/nlu_cw2/results/q4/checkpoint_best.pt
[2021-02-19 04:18:56] Output 500 translations to /tmp/nlu_cw2/results/q4/model_translations.txt
BLEU = 9.69, 40.4/13.1/6.1/3.2 (BP=0.961, ratio=0.962, hyp_len=6056, ref_len=6295)
