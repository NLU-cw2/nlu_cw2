Commencing training!
COMMAND: train.py --save-dir /tmp/nlu_cw2/results/q7 --log-file /tmp/nlu_cw2/results/q7/log.out --data /tmp/nlu_cw2/europarl_prepared --arch transformer
Arguments: {'data': '/tmp/nlu_cw2/europarl_prepared', 'source_lang': 'de', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 10, 'train_on_tiny': False, 'arch': 'transformer', 'max_epoch': 100, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 10, 'log_file': '/tmp/nlu_cw2/results/q7/log.out', 'save_dir': '/tmp/nlu_cw2/results/q7', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 128, 'encoder_ffn_embed_dim': 512, 'encoder_layers': 2, 'encoder_attention_heads': 2, 'decoder_embed_dim': 128, 'decoder_ffn_embed_dim': 512, 'decoder_layers': 2, 'decoder_attention_heads': 2, 'dropout': 0.1, 'attention_dropout': 0.2, 'activation_dropout': 0.1, 'no_scale_embedding': False, 'device_id': 0}
Loaded a source dictionary (de) with 5047 words
Loaded a target dictionary (en) with 4420 words
Built a model with 2707652 parameters
Epoch 000: loss 5.069 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 28.36 | clip 1
Epoch 000: valid_loss 4.34 | num_tokens 13.8 | batch_size 500 | valid_perplexity 76.6
Epoch 001: loss 4.167 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 31.24 | clip 1
Epoch 001: valid_loss 3.96 | num_tokens 13.8 | batch_size 500 | valid_perplexity 52.4
Epoch 002: loss 3.76 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 33.62 | clip 1
Epoch 002: valid_loss 3.76 | num_tokens 13.8 | batch_size 500 | valid_perplexity 43
Epoch 003: loss 3.452 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 35.63 | clip 1
Epoch 003: valid_loss 3.63 | num_tokens 13.8 | batch_size 500 | valid_perplexity 37.7
Epoch 004: loss 3.198 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 37.69 | clip 1
Epoch 004: valid_loss 3.53 | num_tokens 13.8 | batch_size 500 | valid_perplexity 34.3
Epoch 005: loss 2.974 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 39.47 | clip 1
Epoch 005: valid_loss 3.48 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32.6
Epoch 006: loss 2.772 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.27 | clip 1
Epoch 006: valid_loss 3.45 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.6
Epoch 007: loss 2.587 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.79 | clip 1
Epoch 007: valid_loss 3.43 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.9
Epoch 008: loss 2.418 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 44.25 | clip 1
Epoch 008: valid_loss 3.44 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.2
Epoch 009: loss 2.259 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 45.79 | clip 1
Epoch 009: valid_loss 3.44 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31.1
Epoch 010: loss 2.11 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 47.01 | clip 1
Epoch 010: valid_loss 3.47 | num_tokens 13.8 | batch_size 500 | valid_perplexity 32.2
Epoch 011: loss 1.98 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 48.31 | clip 1
Epoch 011: valid_loss 3.5 | num_tokens 13.8 | batch_size 500 | valid_perplexity 33.1
Epoch 012: loss 1.853 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 49.28 | clip 1
Epoch 012: valid_loss 3.55 | num_tokens 13.8 | batch_size 500 | valid_perplexity 34.7
Epoch 013: loss 1.737 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 50.28 | clip 1
Epoch 013: valid_loss 3.58 | num_tokens 13.8 | batch_size 500 | valid_perplexity 36
Epoch 014: loss 1.63 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 51.07 | clip 1
Epoch 014: valid_loss 3.61 | num_tokens 13.8 | batch_size 500 | valid_perplexity 37.1
Epoch 015: loss 1.534 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 51.74 | clip 0.999
Epoch 015: valid_loss 3.67 | num_tokens 13.8 | batch_size 500 | valid_perplexity 39.4
Epoch 016: loss 1.445 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 52.4 | clip 1
Epoch 016: valid_loss 3.73 | num_tokens 13.8 | batch_size 500 | valid_perplexity 41.7
Epoch 017: loss 1.364 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 52.77 | clip 1
Epoch 017: valid_loss 3.78 | num_tokens 13.8 | batch_size 500 | valid_perplexity 44
No validation set improvements observed for 10 epochs. Early stop!
[2021-02-19 21:02:33] COMMAND: translate.py --checkpoint-path /tmp/nlu_cw2/results/q7/checkpoint_best.pt --output /tmp/nlu_cw2/results/q7/model_translations.txt
[2021-02-19 21:02:33] Arguments: {'cuda': False, 'seed': 42, 'data': '/tmp/nlu_cw2/europarl_prepared', 'checkpoint_path': '/tmp/nlu_cw2/results/q7/checkpoint_best.pt', 'batch_size': 10, 'output': '/tmp/nlu_cw2/results/q7/model_translations.txt', 'max_len': 25, 'source_lang': 'de', 'target_lang': 'en', 'max_tokens': None, 'train_on_tiny': False, 'arch': 'transformer', 'max_epoch': 100, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 10, 'log_file': '/tmp/nlu_cw2/results/q7/log.out', 'save_dir': '/tmp/nlu_cw2/results/q7', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 128, 'encoder_ffn_embed_dim': 512, 'encoder_layers': 2, 'encoder_attention_heads': 2, 'decoder_embed_dim': 128, 'decoder_ffn_embed_dim': 512, 'decoder_layers': 2, 'decoder_attention_heads': 2, 'dropout': 0.1, 'attention_dropout': 0.2, 'activation_dropout': 0.1, 'no_scale_embedding': False, 'device_id': 0, 'max_src_positions': 512, 'max_tgt_positions': 512}
[2021-02-19 21:02:33] Loaded a source dictionary (de) with 5047 words
[2021-02-19 21:02:33] Loaded a target dictionary (en) with 4420 words
[2021-02-19 21:02:33] Loaded a model from checkpoint /tmp/nlu_cw2/results/q7/checkpoint_best.pt
[2021-02-19 21:02:41] Output 500 translations to /tmp/nlu_cw2/results/q7/model_translations.txt
BLEU = 10.63, 41.6/15.3/7.1/3.5 (BP=0.946, ratio=0.947, hyp_len=5962, ref_len=6295)
